Preparation:

Put the login.pem file in ~/data (this location is currently hardcoded)

To create the "LAN" experiment with $C clients:

C=...

exp="exp.1lan.$C"
./bin/mknet L=$C EXP=$exp [other options, see below]

To create the "Two LANs" experiment with $C clients:

C=...

exp="exp.2lan.$C"
./bin/mknet L=$C R=1 EXP=$exp [other options, see below]

To create the "Generic experiment with $L clients per LAN and $R router levels
($R >= 2):

L=...
R=...

exp="exp.generic.$R.$L"
./bin/mknet L=$L R=$R EXP=$exp [other options, see below]

other options:
    TIME=180                 time in minutes before the testbed expires,
                             default is 120 (jfed's default)
    AUTH=vwall1              change autority (default: wvall2)
    AUTH=grid5000            Change authority to GRID5000 (Separate testbed)
    PWD=~/data/login.pwd     file containing password for login.pem
    K=~/.ssh/id_rsa.pub      add another allowed ssh key to the testbed
                             (the one from login.pem and the one in the
                             "objects" directory are added automatically)
    Klibrecas=~/data/id_rsa.pub
                             Add key for non-default user (here "librecas")

If no other keys are specified with "K=", the default key from login.pem
as well as the private key in the "objects" directory can be used to log
in to the testbed: the latter key is also used by the nodes to talk to
each other.

xvfb-run java -Djdk.gtk.version=2 -jar ~/jfed_cli/experimenter-cli2.jar \
    -a /var/tmp/$exp/action.yaml

This takes about 10 minutes, then it stops without saying anything;
to check progress:

tail /var/tmp/$exp/debug.txt

To prepare the testbed for the experiment using $SIZES data sizes
(a comma-separate list, in megabytes)

SIZES=...

./bin/setup-experiment $SIZES $exp

This currently copies all the information to the testbed but does not set
things up and run the experiment.  Log in to the "director":

./bin/ssh-experiment $exp director0

On the director itself, start by setting up all the other nodes:

/tmp/experiment/setup-all

And then run the experiment:

/tmp/experiment/run-experiment

Alternatively to leave the experiment running until the testbed expires:

while true; do /tmp/experiment/run-experiment; done

To log in to any other node:

./bin/ssh-experiment $exp NODE_NAME

To run a command using ssh:

./bin/ssh-experiment $exp NODE_NAME PROGRAM [ARGUMENTS]

If the scripts have been edited locally, they can be updated on the testbed
by re-running setup-experiment locally, then asking director0 to copy
them to all other nodes:

./bin/setup-experiment SIZES $exp
./bin/ssh-experiment $exp director0 /tmp/experiment/copy-experiment

To copy data to/from a node in the experiment, there are two scripts,
rsync-to-experiment and rsync-from-experiment, called as:

./bin/rsync-to-experiment EXP_NAME NODE RSYNC_OPTIONS LOCAL_SOURCE [LOCAL_SOURCE]... REMOTE_DEST
./bin/rsync-from-experiment EXP_NAME NODE RSYNC_OPTIONS REMOTE_SOURCE [REMOTE_SOURCE]... LOCAL_DEST

for example after building new binaries in /tmp/binaries on director0 one
could copy them back to the objects directory with:

./bin/rsync-from-experiment $exp director0 -avHP /tmp/binaries/ objects/

or for more complicated rsync options:

./bin/rsync-from-experiment $exp director0 '-avHP --delete' /tmp/binaries/ objects/

==============================================

To quickly average some data from the experiment, the "averages" script
may help.  It reads the result tarballs and extracts the required data
into a sqlite database, then calculates averages, minimum, maximum and
standard deviation.  The database is considered a cache, so if the same
tarball is read back in the script will not read the tarball at all.
Currently, it can determine the total time taken by clients to download
the update, but the script can be extended to collect other data.

./bin/averages [OPTIONS] DATA_NAME CACHE_DATABASE TARBALL [TARBALL]

DATA_NAME is the data item to extract: currently it can only be "run-time"

OPTIONS can be one or more of:

    -r   show data (e.g. 123.456 seconds instead of 2:03.456)
    -a   sort by average value (default)
    -m   sort by maximum value
    -n   sort by experiment name ("UPDATE SCHEDULE SIZE")

for example, to compare the running time of all "2GB" experiments sorting
by maximum update time (assuming the tarballs are in ~/results and the
file names are as produced by the experiment):

./bin/average -m run-time /tmp/cache.sqlite ~/results/*-32-*.tar.gz

==============================================

To extract the data produced by the experiments to a set of files in a
directory, if the data is in $SRC_DIR and the results will go into
$DST_DIR:

./bin/extract-data "$SRC_DIR" "$DST_DIR" '*'

Or to extract only some files matching PATTERNS:

./bin/extract-data "$SRC_DIR" "$DST_DIR" PATTERNS

The extracted files in $DST_DIR will have names like

NODE.EXPERIMENT.SCHEDULING.SIZE.TOPOLOGY.TIMESTAMP

for example:

server0.multicast.immediate.128.S1R2L2.20211219135844

These are lwmon binary files; to display the contants (install lwmon and) type:

lwmon -P- -R "$DST_DIR"/server0.multicast.immediate.128.S1R2L2.20211219135844

See lwmon(1) for other options

Alternatively, the data can be imported into a SQLite database with:

./bin/import-into-sqlite database.sqlite "$DST_DIR"/*

The database will contain tens of millions of rows if importing the full
experiment results, and will require about 4GB of free space.  Once this
has been created, the following command will extract a "summary" of
all the data we measure collected by experiment type and number of
active clients:

./bin/lookup-result database.sqlite all out=summary.sqlite

the same program can also select a subset of the data (replaceing the "all"
with items like server:load or router:tx (bytes transmitted), and it can
just print the numbers on standard output rather than creating a database
by omitting the "Out=" line (this is why it's called "lookup-result",
originally it was meant to look up a specific experiment).

Another tool allows to average the information from the summary database
and just print things like server CPU usage versus number of clients
active with something like:

./bin/get-summary summary.sqlite server usercpu (experiment types, schedules, sizes)

e.g. for multicast and scp experiments, "immediate" scheduling and 2048 MB:

./bin/get-summary summary.sqlite server usercpu multicast scp immediate 2048

For a comprehensive set of summary data given the database with all inputs
in ~/DB/jfed.sqlite:

mkdir -p /tmp/jfed

# Rebuild summary DB:
rm ~/DB/summary.sqlite
lookup-result ~/DB/jfed.sqlite all out=~/DB/summary.sqlite
net-totals ~/DB/jfed.sqlite ~/DB/summary.sqlite

# "by testbed" files:
mkdir /tmp/jfed/by-number-and-testbed /tmp/jfed/totals-by-number-and-testbed
get-summary ~/DB/summary.sqlite instance an server:totcpu server:mem \
  server:tx ae as az out=/tmp/jfed/by-number-and-testbed/%s.txt \
  group=schedule,datasize,topology,instance
get-summary ~/DB/summary.sqlite instance an server:tot_tx client:update_time \
  ae as az out=/tmp/jfed/totals-by-number-and-testbed/%s.txt \
  group=schedule,datasize,topology,instance

# "by topology" files:
mkdir /tmp/jfed/by-number-and-topology /tmp/jfed/totals-by-number-and-topology
get-summary ~/DB/summary.sqlite an server:totcpu server:mem server:tx ae as \
  az out=/tmp/jfed/by-number-and-topology/%s.txt \
  group=schedule,datasize,topology
get-summary ~/DB/summary.sqlite an server:tot_tx client:update_time ae as \
  az out=/tmp/jfed/totals-by-number-and-topology/%s.txt \
  group=schedule,datasize,topology

# "by number of clients" files:
mkdir /tmp/jfed/by-number /tmp/jfed/totals-by-number
get-summary ~/DB/summary.sqlite server:totcpu server:mem server:tx ae as az \
  out=/tmp/jfed/by-number/%s.txt group=schedule,datasize
get-summary ~/DB/summary.sqlite server:tot_tx client:update_time ae as az \
  out=/tmp/jfed/totals-by-number/%s.txt group=schedule,datasize

# "all runs" files:
mkdir /tmp/jfed/runs /tmp/jfed/run_totals
lookup-result ~/DB/jfed.sqlite server:tx server:load \
  server:totcpu server:mem txt=/tmp/jfed/runs/%s.txt
net-totals ~/DB/jfed.sqlite ~/DB/summary.sqlite /tmp/jfed/run_totals/%s.txt


# How to run the experiment

These instructions relate to running this experiment on a testbed managed
with jfed.  We use experimenter-cli2 as the networks are generated by
scripts and it's easiest to provide them to a command-line tool.

It is recommended to have a copy of the report handy as it contains a
more complete description of the experiment.

## Preparation

Set up a folder accessible using rsync which will receive experiment results.
Note the rsync URL for later reference.

The repository contains objects ready to run on a testbed using Ubuntu 20.04;
they may work on other distributions or other versions, but they may also need
to be rebuilt for a different testbed; if that is the case, obtain all sources
and put them inside the "objects" directory for later use:

```
  mkdir objects/sources
  (cd objects/sources && git clone https://github.com/librestack/librecast)
  (cd objects/sources && git clone https://github.com/librestack/iotupd)
  (cd objects/sources && git clone https://github.com/librestack/unisync)
  (cd objects/sources && git clone https://github.com/librestack/lcroute)
  (cd objects/sources && git clone https://github.com/librestack/lwmon)
```

Make sure a "plain text" copy of the login certificate is accessible to the
user running the experiment. The scripts would accept a passphrase-protected
version, but it turns out that a byg in jfed means it cannot be used.

For simplicity, create a file .config/librecast/mknet and put the location
of the login certificate in it as follows:

```
  mkdir -p $HOME/.config/librecast
  echo "LOGIN=~/experiment/login.plain" >> $HOME/.config/librecast/mknet
```

(change the "echo" to point at the file, or use your favourite editor instead
of echo).  See below for other options which can be added to the configuration
file or the command lines

## Creating an experiment network

To create the "LAN" experiment with $C clients:

```
  ./bin/mknet L=$C [other options, see below]
```

for example, for 40 clients:

```
  ./bin/mknet L=40 [other options, see below]
```

To create the "Two LANs" experiment with $C clients:

```
  ./bin/mknet L=$C R=1 [other options, see below]
```

For example, with 32 clients:

```
  ./bin/mknet L=32 R=1 [other options, see below]
```

To create the "Generic experiment with $L clients per LAN and $R router levels
($R >= 2):

```
  ./bin/mknet L=$L R=$R [other options, see below]
```

For example, 3 router levels, 5 clients per lan (for a total of 20 clients
divided between 4 LANs):

```
  ./bin/mknet L=5 R=3 [other options, see below]
```

other options can be provided in the command line or in the configuration file
$HOME/.config/librecast/mknet (case is actually unimportant, so "time=180" is
the same as ("TIME=180"):

* TIME=180
  time in minutes before the testbed expires, default is 120 (jfed's default)
* AUTH=vwall1
  change autority (default: wvall2)
* AUTH=grid5000
  Change authority to GRID5000 (Separate testbed)
* SUFFIX=A
  Add a suffix to the experiment name, which is generated from the number
  of routers and clients, if needed to make the name unique in the testbed
* DIR=/tmp
  directory where the experiment data will be saved, default /var/tmp (each
  experiment will be saved in a subdirectory named after the experiment itself)
* LOGIN=~/data/login.pem
  login credentials, plain text or protected by password (the latter may not
  work with jfed-cli)
* PWD=~/data/login.pwd
  file containing password for login.pem
* K=~/.ssh/id\_rsa.pub
  add another allowed ssh key to the testbed (the one from login.pem and
  the one in the "objects" directory are added automatically); this can
  be repeated as many times as needed
* Klibrecas=~/data/id\_rsa.pub
  Add key for non-default user (here "librecas"); this can be repeated
  as many times as needed
* U=username
  add a (fed4fire) username to the list of users allowed to see this
  experiment; this can be repeated as necessary; note that jfed-cli does
  not appear to respect this and other users may not be able to see
  the experiments

The experiment name is generated using the number of servers, routers and
clients, and suffixed by the optional suffix, using a format like:

```
  S${SERVERS}L${CLIENTS}${SUFFIX}               (no routers)
  S${SERVERS}R${ROUTERS}L${CLIENTS}${SUFFIX}    (routers present)
```

This can be overridden by adding "EXP=NAME" to the command line

If no other keys are specified with "K=", the default key from login.pem
as well as the private key in the "objects" directory can be used to log
in to the testbed: the latter key is also used by the nodes to talk to
each other.  Note that the key in the "objects" directory is not secure in
any way, since it appears in a public repository, and it's only used by
the nodes talking to each other.

For ease of copy and paste, set the shell variable "exp" to the name of
the experiment, for example:

```
  exp=S1L40
  exp=S1R1L32
  exp=S1R1L5
```

The program creates several files in /var/tmp/$exp (of the directory
specified with DIR= instead of /var/tmp), and also writes a summary of
what it has done on standard output.  To create the testbed:

```
  xvfb-run java -Djdk.gtk.version=2 -jar ~/jfed_cli/experimenter-cli2.jar \
      -a /var/tmp/$exp/action.yaml
```

with the paths modified as required for the local install.

This takes about 10 minutes, then it stops without saying anything;
to check progress:

```
  tail /var/tmp/$exp/debug.txt
```

To prepare the testbed for the experiment using $SIZES data sizes
(a comma-separate list, in megabytes)

```
  ./bin/setup-experiment $SIZES $exp
```

For example, we always used 32, 128 and 512 megabytes as well as 2
gigabytes:

```
  ./bin/setup-experiment 32,128,512,2048 $exp
```

This currently copies all the information to the testbed but does not set
things up and run the experiment.  Log in to the "director":

```
  ./bin/ssh-experiment $exp director0
```

If needed, build new binaries at this point: see below for instructions.

On the director itself, start by setting up all the other nodes:

```
  /tmp/experiment/setup-all
```

And then run the experiment:

```
  /tmp/experiment/run-experiment --rsync DESTINATION
```

where DESTINATION is the rsync URL which will receive results.

Alternatively to leave the experiment running until the testbed expires:

```
  while true; do /tmp/experiment/run-experiment --rsync DESTINATION; done
```

To log in to any other node:

```
  ./bin/ssh-experiment $exp NODE_NAME
```

To run a command using ssh:

```
  ./bin/ssh-experiment $exp NODE_NAME PROGRAM [ARGUMENTS]
```

If the scripts have been edited locally, they can be updated on the testbed
by re-running setup-experiment locally, then asking director0 to copy
them to all other nodes:

```
  ./bin/setup-experiment $SIZES $exp
  ./bin/ssh-experiment $exp director0 /tmp/experiment/copy-experiment
```

To copy data to/from a node in the experiment, there are two scripts,
rsync-to-experiment and rsync-from-experiment, called as:

```
  ./bin/rsync-to-experiment $exp NODE RSYNC_OPTIONS \
    LOCAL_SOURCE [LOCAL_SOURCE]... REMOTE_DEST
  ./bin/rsync-from-experiment $exp NODE RSYNC_OPTIONS \
    REMOTE_SOURCE [REMOTE_SOURCE]... LOCAL_DEST
```

for example after building new binaries in /tmp/binaries on director0 one
could copy them back to the objects directory with:

```
  ./bin/rsync-from-experiment $exp director0 -avHP /tmp/binaries/ objects/
```

or for more complicated rsync options:

```
  ./bin/rsync-from-experiment $exp director0 '-avHP --delete' /tmp/binaries/ objects/
```

## Building new binaries

If it is necessary to build new binaries, log in to director0 and build
and install the librecast library first, as other sources will depend on
it:

```
  ./bin/ssh-experiment $exp director0
  cd /tmp/experiment/sources/librecast
  make
  make DESTDIR=/tmp/binaries install
  cp -a /tmp/binaries/usr/include/librecast.h /tmp/experiment
  cp -a /tmp/binaries/usr/include/librecast/*.h /tmp/experiment/librecast
  cp -a /tmp/binaries/usr/lib*/liblibrecast* /tmp/experiment
  /tmp/experiment/setup-node director 0
```

The last command installs librecast library and include files on director0
itself but not on all nodes yet (note that there is a space between
director and 0 in that command).

Then build the other sources:

```
  cd /tmp/experiment/sources/lwmon
  make
  cp -a obj/bin/* obj/build/objects/liblwmon* include/lwmon.h /tmp/experiment
  cp -a include/lwmon/* /tmp/experiment/lwmon-include

  cd /tmp/experiment/sources/lcroute
  make
  cp -a src/lcroute /tmp/experiment

  cd /tmp/experiment/sources/iotupd
  make
  cp -a src/iotup[cd] /tmp/experiment

  cd /tmp/experiment/sources/unisync
  make
  cp -a src/iotup /tmp/experiment
```

Now install these objects to all nodes:

```
  /tmp/experiment/setup-all
```

Finally, copy the objects back to your local system so they can be used with
future experiments:

```
  ./bin/rsync-from-experiment $exp director0 '-avHP' /tmp/experiment/ objects/
```

## Processing results
Please refer to the report for more complete information; this is
just a quick list of commands.

To quickly average some data from the experiment, the "averages" script
may help.  It reads the result tarballs and extracts the required data
into a sqlite database, then calculates averages, minimum, maximum and
standard deviation.  The database is considered a cache, so if the same
tarball is read back in the script will not read the tarball at all.
Currently, it can determine the total time taken by clients to download
the update, but the script can be extended to collect other data.

```
  ./bin/averages [OPTIONS] DATA_NAME CACHE_DATABASE TARBALL [TARBALL]
```

`DATA_NAME` is the data item to extract: currently it can only be "run-time",
"server-load" or "server-tx" although adding more is very easy.

`OPTIONS` can be one or more of:

* -r
  show raw data (e.g. 123.456 seconds instead of 2:03.456)
* -a
  sort by average value (default)
* -m
  sort by maximum value
* -n
  sort by experiment name ("UPDATE SCHEDULE SIZE")
* -s
  sort by data size, then experiment name ("UPDATE SCHEDULE SIZE")
* -k
  sort by number of active clients (where applicable)
* -c
  do not display data, only update cache database
* -p
  interpret TARBALL as a "prefix" (like adding a "\*" to the end but expanded
  by the script and not the shell, and also excluding incomplete and test runs)
* -SSCHEDULE
  with "-p", only select tarballs produced by SCHEDULE ("immediate", "random"
  or "random2")
* -UUPDATE
  with "-p", only select tarballs produced by UPDATE ("multicast", "scp",
  "tcp" or "udp")
* -ZSIZE
  with "-p", only select tarballs produced by experiments with the given
  data size
* -T
  produce pre-formatted text (default)
* -H
  produce HTML table
* -L
  produce LaTeX tabular environment

for example, to compare the running time of all "2GB" experiments sorting
by maximum update time (assuming the tarballs are in ~/results and the
file names are as produced by the experiment):

```
  ./bin/average -m run-time /tmp/cache.sqlite ~/results/*-2048-*[0-9].tar.gz
```

(the "[0-9]" is a quick way to exclude names which don't end in a
numeric timestamp, for example incomplete and test runs)

and to summarise all runs from experiment S1L49F:

```
  ./bin/average -m -p run-time /tmp/cache.sqlite ~/results/S1L49F-
```

in this case, using "-p" simplifies the command line

To extract the data produced by the experiments to a set of files in a
directory, if the data is in `$SRC_DIR` and the results will go into
`$DST_DIR`:

```
  ./bin/extract-data "$SRC_DIR" "$DST_DIR" '*'
```

Or to extract only some files matching PATTERNS:

```
  ./bin/extract-data "$SRC_DIR" "$DST_DIR" PATTERNS
```

The extracted files in `$DST_DIR` will have names like

  NODE.EXPERIMENT.SCHEDULING.SIZE.TOPOLOGY.TIMESTAMP

for example:

  server0.multicast.immediate.128.S1R2L2.20211219135844

These are lwmon binary files; to display the contants (install lwmon and) type:

```
  lwmon -P- -R "$DST_DIR"/server0.multicast.immediate.128.S1R2L2.20211219135844
```

See lwmon(1) for other options


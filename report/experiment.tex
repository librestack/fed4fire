\documentclass[a4paper,12pt]{article}

\usepackage{ifpdf}

\title{Comparing performance of IPv6 multicast and unicast for software updates}
\author{Claudio Calvelli}
\date{Draft 0.4 \today}

\setlength{\unitlength}{0.01\linewidth}
\def\topfaction{0.6}
\def\bottomfraction{0.6}

\newcommand{\TODO}[1]{\par\noindent%
\hspace*{\fill}%
\framebox{\parbox{0.9\linewidth}{{\bf TODO: }#1}}%
\hspace*{\fill}%
}

% everybody say to just include the hyperref package - it's big, ugly and
% breaks the next command.  This on the other hand works just fine
\ifpdf
\newcommand{\url}[1]{%
\pdfstartlink %
attr {/Border [0 0 0]} %
user {/Subtype /Link /A << /S /URI /URI (#1) >>}%
{\tt #1}\pdfendlink}
\else
\newcommand{\url}[1]{{\tt #1}}
\fi

\newcommand{\pref}[1]{%
\ref{#1}%
\ifnum\thepage=0\pageref{#1}\else\ on page~\pageref{#1}\fi%
}

\begin{document}
\maketitle

\begin{abstract}
The librecast project states that ``Multicast is, by definition, the most
efficient way for multiple nodes to communicate''.  This experiment is
designed to provide evidence of this efficiency by comparing three
methods of sending the same data to a large number of nodes, as
would for example happen when a software update is released. The three
methods are: traditional unicast; a network taking full advantage of
multicast; and finally a hybrid designed to represent the use of transitional
technology to address the lack of multicast routing in many networks.

\TODO{Report the actual results after we run the experiment}
\end{abstract}

\section{Introduction}
The very first paragraph of RFC 3170 states:

\begin{quotation}
\noindent
IP Multicast will play a prominent role on the Internet in the coming
years.  It is a requirement, not an option, if the Internet is going
to scale.  Multicast allows application developers to add more
functionality without significantly impacting the network.
\end{quotation}

There is a need for some experimental data to back these statements.
We concentrate of measuring the impact of {\it software
updates\/} on the network, because the proliferation of connected
devices will make software updates a very important target for
efficient use of the network, and because software updates are
easy to simulate realistically by measuring the impact of copying
a large file to a large number of nodes.

To provide evidence for the above statements, we compare the
following methods of providing updates:

\begin{itemize}
\item Traditional unicast using a TCP-based service: a
server listens to TCP requests to send a copy of the software
update, and each client requests
the update from the server: this is the mechanism used by the vast
majority of current services.
\item Full multicast: a number of servers provide the software
update using multicast, and clients will obtain the updates by
joining a multicast group and waiting for the data to arrive.
\item Unicast using a UDP-based service: this is similar to the
TCP case, but uses datagrams instead of virtual circuits: this
mechanism is introduced because multicast is by necessity based
on datagrams--there is no feedback from receiver to sender--and
we want to help determine which differences may be caused by
unicast vs.\ multicast, and which ones by virtual circuits vs\.
datagrams.
\end{itemize}

Additionally, we also measure the effect of a simple ``scp''
of the file, to confirm that the setup works with standard,
well-tested tools.

Apart from the ``scp'' runs, do not use encryption in this experiment,
all the three methods send the data unencrypted and verify that it
has arrived correctly using a secure hash: this corresponds to the way
some software updates are distributed, with an HTTP mirror providing
the data and a secure hash provided by some more secure mechanism; we
do not expect the results to be different when excription is used for
all transmissions, as used in many other cases, but we might consider
a future experiment to test this.

Independently of the method selected, there are two ``client''
scheduling strategies:

\begin{itemize}
\item All clients request updates at approximately the same time
(the ``immediate'' strategy).
\item Clients wait a random time before requesting the update
(the ``random'' strategy).
\end{itemize}

We run the simulated software updates in a variety of network
configurations and with a variety of file sizes to simulate the
impact of different types of updates; in each case we measure network
use, server load, client load and speed of update for each combination
of update mechanism and scheduling strategy.

The rest of this report is structured as follows:

Section~\ref{LAN:experiment} describes the simplest possible network
topology in which we can get useful measurements, and provides details
on how we run the experiment.

Sections~\ref{TWOLAN:experiment} and~\ref{GEN:experiment} describe
two more network topologies, incresing the complexity and studying
how different features affect the results.

Section~\ref{results} analyses the result of the experiment and compares
the efficiency of unicast, multicast, and the transitional technology.
There are also notes about testbed issues we identified, because anybody
wishing to repeat the experiment will need to make sure they select
an experiment testbed which is properly configured for IPv6 multicast.

Section~\ref{future} explores the possibility of further experiments,
to make the simulation more realistic and more complete.

Appendix~\ref{programs} provides some more details about the various
programs which ran as part of the experiment, and where to find the
full sources of all these programs.

\section{The ``LAN'' experiment}
\label{LAN:experiment}

A number of clients (denoted by ${\cal C}$) request software updates
from a single server; the server and all clients share a LAN
so that updates have the shortest possible netowrk path.  This
experiment will allow us to compare multicast and unicast in the
simplest possible setting, and one which is possible on any existing
local network in which IPv6 is enabled, and could represent for example
distributing updates within an organisation.

A second experimental parameters indicates the size of the software updates
as the number ${\cal N}$ of bytes contained in it.  In the real world,
clients may be more or less up-to-date so that each one may request a
subset of all updates available; for this experiment we assume that
all clients have all previous updates and are just requesting the latest
one; a future experiment may consider some more complex ``real life''
scenarios.

Figure~\pref{s1:c8}
\begin{figure}[bp]
\begin{center}
\begin{picture}(90,29)
\put(45,22){\makebox(0,0){\rm\bf S}}
\put(45,16){\line(0,1){3}}
\put(10,16){\line(1,0){70}}
\multiput(10,13)(10,0){8}{\line(0,1){3}}
\multiput(10,10)(10,0){8}{\makebox(0,0){\rm\bf C}}
\end{picture}
\end{center}
\hspace*{\fill}%
\mbox{{\bf S} = server;}%
\hspace*{\fill}%
\mbox{{\bf C} = client}%
\hspace*{\fill}
\caption{Network with8 clients on a single LAN}
\label{s1:c8}
\end{figure}
shows the network topology with ${\cal C} = 8$, i.e.\ there is a
server sending data to 8 clients on a single LAN.

\subsection{Experiment procedure}
For a specified network topology (i.e.\ value for ${\cal C}$), and a list
of update sizes (several distinct values for ${\cal N}$), we implement
that network on an experiment testbed, then run a series of tests on
it using all possible combinations of update size, update method and
scheduling strategy.  For organisational reasons, the testbed actually
has 2 servers, the extra server does not take part in the update but
directs the operations and collects results (we call this extra server
the ``director'').

Each test starts with the director generating a file of the specified size
filled with random data; this is copied to the server (a future experiment
will use multiple servers, so generating the file on the director and
copying it to all servers will make sure all send the same data, and
in preparation for that we have this extra file copy instead of generating
a random file directly on the server).

After generating and copying the file the director waits 60 seconds to
make sure that the 1-minute load average of each node in the system is
down to its baseline value; when we ran experiments without this wait,
we had each run affecting the measurements of the next one, so it did
not produce useful results.

After the 60 seconds wait, the director asks the server node to start:
this means starting two daemons, a resource monitoring tool and the
update provider appropriate for the selected update method.  These
update providers are described below.

After the server has started, the director asks all client nodes to
start as well: each client will first wait a short time depending on
the scheduling strategy: a fixed 4 seconds for ``immediate'' or a
random duration between 4 and 36 seconds for ``random''; the minimum
wait of 4 seconds is to make sure the rest of the system is fully
ready to run. After this delay, each client starts a resource monitoring
daemon identical to the one running on the server, and a client
program to obtain the update.

When a client has successfully obtained the update, the monitoring
daemon will record the time it has taken, finish another round of
resource measurements, and sends all the data back to the director.

The director waits for all clients to have sent the data, then asks the
server to stop, which will also trigger a copy of the server's resource
measurements back to the director: all the measurements from all nodes
are collected into a single ``tar'' archive and saved for later analysis.

The resource monitoring daemon records the following data every second:

\begin{itemize}
\item 1-minute load average as provided by the system
\item memory and swap use
\item bytes sent and received on the network interface used to
transfer the update data
\end{itemize}

Additionally, at the end of the experiment it also records the following
data about the update program itself (update provider for server, or
the program obtaining the update for clients):

\begin{itemize}
\item Time elapsed between start and termination of the program, in milliseconds
\item CPU time used by the program itself, in milliseconds
\item CPU time used by the operating system to run the program, in milliseconds
(this includes, for example, time used to obtain data from disk)
\item The termination status: whether the program reported an error
\end{itemize}

Since the experiment procedure is automated by running a single program
on the director, where possible we ran it many times on the same
testbed, to have more experimental data without the extra overhead
of setting up a new testbed.

\subsection{Update methods}
The ``multicast'' experiment uses the ``IoT updater'' demonstration
program~\cite{iotupd:paper} to copy a file from server to clients: on the server
side ``iotupd'' runs a loop in which it sends the whole file to a specified
multicast group, then repeats the sending until the end of the experimend.
Each client runs ``iotupc'' which waits for the data to arrive on the
specified group and saves it to a local file, stopping when the file checksum
matches.  There is no feedback from client to server, and no mechanism for
a client to require retransmission of missing data; however the client
can just wait for the server to send it again. More details on the
IoT updater can be found in appendix~\ref{iotupd} or in the source
code~\cite{iotupd:sources}.

\TODO{describe the unicast update methods in more detail}

\section{The ``Two LANs'' experiment}
\label{TWOLAN:experiment}

Similar to the previous experiment, but we investigate the effect of a
multicast router in the network: there are two LANs connected together
by a single router; the server is on the first LAN, and all the clients
are on the second LAN.  Like the previous experiments, the parameters are
the number of clients ${\cal C}$, and the update size in bytes ${\cal N}$.

Figure~\pref{s1:r1:c8}
\begin{figure}[bpt]
\begin{center}
\begin{picture}(90,38)
\put(45,31){\makebox(0,0){\rm\bf S}}
\put(45,25){\line(0,1){3}}
\put(45,22){\makebox(0,0){\rm\bf R}}
\put(45,16){\line(0,1){3}}
\put(10,16){\line(1,0){70}}
\multiput(10,13)(10,0){8}{\line(0,1){3}}
\multiput(10,10)(10,0){8}{\makebox(0,0){\rm\bf C}}
\end{picture}
\end{center}
\hspace*{\fill}%
\mbox{{\bf S} = server;}%
\hspace*{\fill}%
\mbox{{\bf R} = router;}%
\hspace*{\fill}%
\mbox{{\bf C} = client}%
\hspace*{\fill}
\caption{Two LANs network with 8 clients}
\label{s1:r1:c8}
\end{figure}
shows the network topology with ${\cal C} = 8$, i.e.\ the same setting
as the previous example (figure~\pref{s1:c8}) but with the clients
separated from the server by a single router.

The experimental procedure is very similar to the previous experiment,
we only describe the differences between them here.

After copying the update data to the server and waiting 60 seconds, the
director will ask the router to start its own resource monitoring, and,
for the multicast experiment, to start a multicast routing daemon.

The server wait 2 seconds before starting, to give the router time to be
fully set up.

After that, the experiment proceeds identically with the director starting
all clients and waiting for results.

\section{The ``Generic'' experiment}
\label{GEN:experiment}

An extension of the previous (``Two LANs'') experiment includes a longer
network path between clients and server; for simplicity, and to generate
the networks automatically, we specify a number of clients per LAN
(denoted by ${\cal L}$) and the length of the network path indicated by
the number of routers in the path, ${\cal R}$. The total number of clients
will be ${\cal C} = {\cal L} * 2^{{\cal R} - 1}$, and the routers form a
tree structure.

A couple of exampels will make this clearer: figure~\pref{s1:r3:l2}
\begin{figure}[bp]
\begin{center}
\begin{picture}(90,62)
\put(45,55){\makebox(0,0){\rm\bf S}}
\put(45,49){\line(0,1){3}}
\put(45,46){\makebox(0,0){\rm\bf R}}
\put(45,40){\line(0,1){3}}
\put(25,40){\line(1,0){40}}
\multiput(25,37)(40,0){2}{\line(0,1){3}}
\multiput(25,34)(40,0){2}{\makebox(0,0){\rm\bf R}}
\multiput(25,28)(40,0){2}{\line(0,1){3}}
\multiput(15,28)(40,0){2}{\line(1,0){20}}
\multiput(15,25)(20,0){4}{\line(0,1){3}}
\multiput(15,22)(20,0){4}{\makebox(0,0){\rm\bf R}}
\multiput(15,16)(20,0){4}{\line(0,1){3}}
\multiput(10,16)(20,0){4}{\line(1,0){10}}
\multiput(10,13)(10,0){8}{\line(0,1){3}}
\multiput(10,10)(10,0){8}{\makebox(0,0){\rm\bf C}}
\end{picture}
\end{center}
\hspace*{\fill}%
\mbox{{\bf S} = server;}%
\hspace*{\fill}%
\mbox{{\bf R} = router;}%
\hspace*{\fill}%
\mbox{{\bf C} = client}%
\hspace*{\fill}
\caption{Network with 8 clients, 2 clients per LAN}
\label{s1:r3:l2}
\end{figure}
shows the network topology with ${\cal R} = 3$ and ${\cal L} = 2$,
so that the 8 clients are organised in 4 separate LANs, with
7 routers forming a tree structure with the server connected to
the root of the tree. For comparison, figure~\pref{s1:r4:l1}
\begin{figure}[bp]
\begin{center}
\begin{picture}(90,71)
\put(45,64){\makebox(0,0){\rm\bf S}}
\put(45,58){\line(0,1){3}}
\put(45,55){\makebox(0,0){\rm\bf R}}
\put(45,49){\line(0,1){3}}
\put(25,49){\line(1,0){40}}
\multiput(25,46)(40,0){2}{\line(0,1){3}}
\multiput(25,43)(40,0){2}{\makebox(0,0){\rm\bf R}}
\multiput(25,37)(40,0){2}{\line(0,1){3}}
\multiput(15,37)(40,0){2}{\line(1,0){20}}
\multiput(15,34)(20,0){4}{\line(0,1){3}}
\multiput(15,31)(20,0){4}{\makebox(0,0){\rm\bf R}}
\multiput(15,25)(20,0){4}{\line(0,1){3}}
\multiput(10,25)(20,0){4}{\line(1,0){10}}
\multiput(10,22)(10,0){8}{\line(0,1){3}}
\multiput(10,19)(10,0){8}{\makebox(0,0){\rm\bf R}}
\multiput(10,13)(10,0){8}{\line(0,1){3}}
\multiput(10,10)(10,0){8}{\makebox(0,0){\rm\bf C}}
\end{picture}
\end{center}
\hspace*{\fill}%
\mbox{{\bf S} = server;}%
\hspace*{\fill}%
\mbox{{\bf R} = router;}%
\hspace*{\fill}%
\mbox{{\bf C} = client}%
\hspace*{\fill}
\caption{Network with 8 clients, 1 client per LAN}
\label{s1:r4:l1}
\end{figure}
shows the same number of clients arranged on 8 separate LANs
(${\cal L} = 1$) so that there is an extra network hop between
clients and servers (${\cal R} = 4$).

This is still a simplified view of a real system, but allows to
extend the previous experiments to different circumstances.  A
future experiment might consider different networks.

The last experimental parameters indicates the size of the software updates
as the number ${\cal N}$ of bytes contained in it.  In the real world,
clients may be more or less up-to-date so that each one may request a
subset of all updates available; for this experiment we assume that
all clients have all previous updates and are just requesting the latest
one; a future experiment may consider some more complex ``real life''
scenarios.

Each experiment run will select a network topology (i.e.\ values
for ${\cal R}$ and ${\cal L}$) and a list of update sizes
${\cal N}$, set up the corresponding network on the experiment
testbed, and proceed in the same way as the previous, ``Two LANs'',
experiment.  While there are more routers in the network, all these
routers do exactly the same as the single router in the previous
experiment, so there is little difference in the actual running.

\section{Experiment results}
\label{results}

\TODO{Show tables, graphs, link to result data--and draw conclusions}

\subsection{Testbed issues}
While running the experiment we have determined that some testbeds
were not configured properly.  Initially we ran on Virtual Wall 2,
which had the issues identified below; after investigation, we
moved all experiments to Virtual Wall 1, which had fewer issues.

There did not appear to be any MLD snooping on the VLANs we were
assigned when creating experiments; on virtual wall 1, all multicast
packets sent by any of the nodes appeared on other nodes in the same
VLAN, independently of the presence of listeners on the nodes: this
was confirmed using tcpdump.

Virtual wall 2, on the other hand, had a more serious problem, in
that packets were forwarded for some groups but not for others,
and this also independently on the presence of listeners, so when
we ran our experiments some worked and some did not, with no indication
of the reason, until we identified this configuration problem with
the switch. This issue is shown with the commands in figure~\pref{broken},
\begin{figure}[bp]
\begin{verbatim}
server0$ yes 'multicast testing' | \
         mcastsend -i `cat /tmp/interface` ff1e::42 4242
client0$ mcastread `cat /tmp/interface` ff1e::42 4242 \
         > /tmp/datafile
client0$ (kill mcastread after a few seconds)
client0$ wc /tmp/datafile
 1430130  2860261 25742336 /tmp/datafile
server0$ (kill mcastsend)

server0$ yes 'multicast testing' | \
         mcastsend -i `cat /tmp/interface` ff1e::42:1234 4242
client0$ mcastread `cat /tmp/interface` ff1e::42:1234 4242 \
         > /tmp/datafile
client0$ (kill mcastread after some time)
client0$ wc /tmp/datafile
 0 0 0 /tmp/datafile
server0$ (kill mcastsend)
\end{verbatim}
\caption{Example of effect caused by a misconfigured switch}
\label{broken}
\end{figure}

The first set of commands shows the effect of sending a stream on a
group which the switch had decided to forward; a client joins in the
middle and receiving packets for a few seconds; some substantial
amount of data is transferred.

The second set of commands is identical except for the multicast
group which is \verb+ff1e::42:1234+ instead of \verb+ff1e::42+ --
there is no reason to expect any difference, and in fact virtual wall
1 and our own systems give the same results as the previous set
of commands.  However, on virtual wall 2 no data was received
even after leaving the programs running for several minutes.

This was not an issue with a single experiment network, we had this
problem every time we had nodes allocated to virtual wall 2.

Running tcpdump on all nodes, we determined that the switch was
forwarding all packets to all nodes for some multicast groups,
and dropping all packets for other groups; this was independent
on the presence of listeners on the nodes.  We haven't been able
to determine (or to find in the documentation) what criteria the
switch used to decide what groups to forward and what groups to
drop, so we decided to use only virtual wall 1 for our experiment.
no data appears in the file, no matter how long one leaves mcastread
running; on virtual wall 1 and on our own systems, we do not observe
any difference due to the group used, so this issue appears to be
limited to the switch connected to virtual wall 2.

In summary, we need to warn anybody wishing to repeat this experiment
to first make sure that the system they are using is properly
configured.

\section{Future work}
\label{future}

Due to time limitations we have only measured network performance
for a small set of regular network topologies generated by varying
some parameters; of course the real world is made up of rather
more irregular topologies and it it would be interesting to investigate
more variations in this area in a future set of experiments.

We also simplified the software update by assuming that all clients
request exactly the same file, rather than a more complex situation
in which every client requests a different subset of all available
updates, due to its own unique update history; while we expect that
multicast will scale really well to this situation, we haven't
ran an experiment to support this.

All the experiments we ran included a single server.  The main point
of this experiment was to show that a single server is sufficient to
provide updates for a large number of clients using multicast, while
unicast will require multiple servers in this case.  However, there
are reasons other than server and network load why one would want
multiple servers, for example reliability: if the single, extremely
efficient, server has a fault, the updates stop; ideally, these
multiple servers will be reachable by completely different network
paths as well.  We think that multicast will help with that too,
for example multiple servers can each send data at a fraction of the
bandwidth, and when all works clients will get the advantage of the
combined output from all servers, with a network or server failure
would automatically result in a corresponding reduction of speed, and
the subsequent recovery or replacement of the faulty parts would
automatically result in the system returning to full speed.  This
claim, of course, needs a separate experiment to justify.

Another type of network activity which can benefit from multicast is
live streaming, where the server will only need to send the stream once;
this case is similar to software updates and probably does not need a
separate experiment; however if several choices of bandwidth and quality
are required the situation is different.  In the unicast case it's obvious
how the sender can provide different quality streams to different clients,
for multicast the simplest answer is to provide several streams with
different quality, with the client subscribing to the one which best
match its requirements: this would save network and server resources
but there may be better way of achieving this result, for example using
layered codecs to send only one copy of the lowest quality stream, then
a second stream with the difference between that and the next highest
quality. We don't know at present if these codecs would involve more
server resources than the re-encoding required to provide multiple
stream with different quality, but in any case we would find it useful
to run another experiment to measure these costs and compare them with
the expected savings in terms of network usage.

For this experiment we transmitted all data unencrypted between the nodes
and used a secure hash to determine whether it was received correctly.
This corresponds to a traditional situation in which HTTP mirrors provide
the data, and a checksum is provided over a more secure mechanism for
verification.  More recently, most systems are moving to HTTPS with the
added overhead of encryption on every communication: we expect that the
benefits of multicast shown in this experiment will continue to apply,
but we have not tested this, and might consider a future experiment in
which we extend the multicast update method to add encryption of all
communication, comparing this with the normal stream encryption used
for example with HTTPS.

\appendix
\section{Programs}
\label{programs}

To run each experiment we had to implement a network topology on an
experiment testbed, set up each node in the testbed, run the experiment
itself and collect the results; additionally, we had to analyse the
results of groups of experiments together.  This appendix describes
the programs used for all various tasks, and includes references to
where the full source code can be found for the programs we developed.

\subsection{Preparing a testbed and running an experiment}

{\em The programs described here and other useful tools are in the experiment
setup repository~\cite{exp:scripts} under the {\tt bin} directory for
the programs to run on the local system and the {\tt objects} directory
for the programs to run on the testbed.}

Given the number of servers, clients and routers (if appropriate to the
experiment) we developed a simple program to generate action files
for ``jfed''~\cite{jfed} so that the process could be automated; a single program
``mknet'' provided action files for all the experiment topologies
described in this report by providing appropriate options; for the
networks shown as examples in the figures we just ran:

\begin{verbatim}
local$ mknet L=8
local$ mknet R=1 L=8
local$ mknet R=2 L=2
local$ mknet R=3 L=1
\end{verbatim}

As can be seen, omitting ``R'' results in a single LAN network in which
the number of clients is specified by ``L'' for consistency with the
other networks (where it indicates the number of clients on each client
LAN).

By default, the program generates an experiment name indicating the
parameters provided: for the four examples above this would be: ``S1L8'',
``S1R1L8'', ``S1R2L2'' and ``S1R3L1''; the data generated will be stored
in a directory inside {\tt/tmp} named after the experiment (the name
of the experiment starts with ``S1'' to indicate the number of servers,
in preparation for a future multi-server experiment).

One of the files generated, {\tt action.yaml}, is suitable for using as
an action file with the ``jfed-cli'' tool and will provision the testbed;
the program also generates {\tt action.rspec} which is suitable for
using with the ``jfed-gui'' tool.  We do not describe these tools here
as they are provided by fed4fire, but see~\cite{jfed}.

Once the testbed is up and running, we need to copy some things to it,
for example the actual programs which will run on it and information about
the experiment to run.  The list of data sizes is also specified at
this point, for example to run with 32, 64 and 512 megabytes on the
first single LAN experiment defined above (``S3L8'')

\begin{verbatim}
local$ setup-experiment 32,64,512 S3L8
\end{verbatim}

This sets up the ``director'' node and copies the {\tt objects} directory
of the repository to it.  To complete the setup, one needs to connect to
it and then run a program there:

\begin{verbatim}
local$ ssh-experiment S3L8 director0
director0$ /tmp/experiment/setup-all
\end{verbatim}

The testbed is now ready to run the experiment by running a program on
director0:

\begin{verbatim}
local$ ssh-experiment S3L8 director0
director0$ /tmp/experiment/run-experiment
\end{verbatim}

If required, the ``{\tt run-experiment}'' program can run many times
to obtain more data without additional setup overhead; no need to
repeat any of the previous steps.

Internally, the ``{\tt run-experiment}'' program calls other programs
running on the director, but also on servers, routers and clients
as necessary to implement the procedure described in
sections~\ref{LAN:experiment} to~\ref{GEN:experiment}. These
have names like ``{\tt start-tcp-server}'' or
``{\tt start-multicast-router}'' to start wat is required for
a particular experiment on a particular node (in this example,
start the TCP experiment on a server, and start the multicast
routing daemon on a router, respectively).  All these programs are
found in the repository cited.

One issue we found while developing programs to automate the experiment
is that network interface names may be different when booting
different testbeds; each node has two interfaces, a control
interface used by the testbed administration, as well as to log
in to it from outside the experiment, and a second interface
connected as required by the experiment's network topology and
used to transfer the data files during the experiment; for a router
node, there are obviously more interfaces.  We did not have time
to develop our own boot image to work around the problem that
systemd's ``predictable'' interface names turn out to be nothing
of the sort, so the ``setup-all'' script will additionally have
to figure out what interface is going to be used for what, and
configure them as required: this may be different for each node.
It will also assign friendlier names to the interface which tell
us what is used for what, as is important when looking at network
usage data for routers.

\subsubsection{Resource monitoring}
{\em The program described here is in the lwmon repository~\cite{lwmon}.}

There are many monitoring tools for Unix system, however in our experience
they tend to use more resources than programs which do the actual work,
or else they are designed to sample information only once a minute,
which is not enough for this experiment.

Because of experience using other monitoring tools and not finding
one which we actually want to use on a live system, we have developed
our own over the years, which we call ``lwmon'', for Light-Weight
system MONitoring, which, as the name suggests, is very considerate
in its use of resources and can safely run very frequently without
impact on the system.  It can also report on its own resource usage,
so one can confirm that it is, indeed, light-weight.

Without going into complete details, each node sets up its own
configuration for lwmon, which then monitors memory usage and
system load every second, its own resource usage every 10
seconds.  The appropriate program (update provider for servers,
routing daemon for routers if requires by the experiment, and
the program which gets the updates for clients) also runs as
a child process of lwmon, so the latter can report on the resource
used by this program.

A lwmon configuration for a client looks like:

\begin{verbatim}
hostname client2

load lavg 1
memory memswap 1
network if1 1 enp6s0
self self 2

program exp 5 /tmp/experiment/start-multicast-client enp6s0
print /tmp/results/client2,multicast,random.64 overwrite binary
\end{verbatim}

This means that load average, memory/swap usage and network usage are
sampled every second, lwmon's own resource usage every 2 seconds, will
run the appropriate program for a particular experiment (in this case,
update via multicast), and save the information into a file in a compact
binary format. The second column of the lines specifying what to measure
is the name used to report it.

The program running does not depend on the scheduling strategy selected,
as that is handled before: we do not want to measure the time it takes
to sleep for a random duration, only the time it takes to download the
update.  The file name for the results, on the other hand, contain the
scheduling strategy (in this case, random) and the update size (64
megabytes), as we need to keep things distinct.

Another thing to note is that the ``network'' line asks to monitor
the interface using its systemd name (enp6s0 in this example) because
that's how it will be able to find it in the system, but reports it
using the name ``if1'' as this is the interface name we have used
in the action file and rspec.

Configuration for a server is essentially identical, with ``client''
replaced by ``server'', and for a router it is very similar:

\begin{verbatim}
hostname router0

load lavg 1
memory memswap 1
network if1 1 enp4s0
network if2 1 enp6s0
self self 2

program exp 5 /tmp/experiment/start-multicast-router enp4s0
print /tmp/results/router0,multicast,random.64 overwrite binary
\end{verbatim}

The network interface reported as ``if1'' has network traffic going
to or from clients, and ``if2'' has traffic going to or from servers.

Once the ``{\tt start-}\ldots'' program terminates, lwmon automatically
reports on its resource usage and the wall clock time it has taken to run,
then runs one more round of measurements and exits.  For clients, the
program normally terminates when it has obtained the update successfully;
for servers and routers the program terminates when the director signals
the end of the experiment.

The program which generates the lwmon configuration file and calls lwmon
will wait for it to terminate, then copies the file it produced back
to the director node. This allows the director to collect all the
data about the experiment in one place.

As mentioned, lwmon produces a file containing data in a packed binary
format. To just look at the data, the tool has an option to read that
binary file back in and produce a human-readable output:

\begin{verbatim}
$ lwmon -R /tmp/results/router0,multicast,random.64 -P -
\end{verbatim}

There is a separate tool which reads one or more data files and produces
SQL statements which can be used to import it into a database, for example:

\begin{verbatim}
$ lwmon-to-sql [options] DATA_FILES | sqlite3 database.sqlite
\end{verbatim}

As described below in~\ref{importing}, for this experiment we have developed
a tool which calls {\tt lwmon-to-sql} in the appropriate way to import
experiment results in a format suitable for further analysis.

\subsubsection{Programs implementing the multicsst update method}
\label{iotupd}
\TODO{iotupd}

\subsubsection{Programs implementing the unicast update methods}
\label{unisync}
\TODO{unisync}

\subsection{Importing results in a database}
\label{importing}
\TODO{extract-data, import-into-sqlite}

\subsection{Analysing results}
\TODO{extract-result, \ldots}

\section*{References}
\bibliographystyle{plain}
\begin{thebibliography}{99}
\begin{sloppypar}

\bibitem{lwmon}
  C. Calvelli.
  {\em lwmon: A {\bf\em l}ight-{\bf\em w}eight system {\bf\em mon}itoring tool}.\\
  \url{https://example.com/TODO-need-url}

\bibitem{exp:scripts}
  C. Calvelli, E. Payne, B. Sheffield.
  {\em Fed4fire experiment setup scripts}.\\
  \url{https://example.com/TODO-need-url}

\bibitem{jfed}
  {\em jfed 5.9 documentation}.\\
  \url{https://doc.ilabt.imec.be/jfed-documentation-5.9/index.html}

\bibitem{iotupd:paper}
  TODO: iotupd paper

\bibitem{iotupd:sources}
  TODO: iotupd sources

\end{sloppypar}
\end{thebibliography}

\end{document}


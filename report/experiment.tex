\documentclass[a4paper,12pt]{article}

\title{Comparing performance of IPv6 multicast and unicast for software updates}
\author{Claudio Calvelli}
\date{Draft 0.3 \today}

\setlength{\unitlength}{0.01\linewidth}

\newcommand{\TODO}[1]{\par\noindent%
\hspace*{\fill}%
\framebox{\parbox{0.9\linewidth}{{\bf TODO: }#1}}%
\hspace*{\fill}%
}

\newcommand{\pref}[1]{%
\ref{#1}%
\ifnum\thepage=0\pageref{#1}\else\ on page~\pageref{#1}\fi%
}

\begin{document}
\maketitle

\begin{abstract}
The librecast project states that ``Multicast is, by definition, the most
efficient way for multiple nodes to communicate''.  This experiment is
designed to provide evidence of this efficiency by comparing three
methods of sending the same data to a large number of nodes, as
would for example happen when a software update is released. The three
methods are: traditional unicast; a network taking full advantage of
multicast; and finally a hybrid designed to represent the use of transitional
technology to address the lack of multicast routing in many networks.

\TODO{Report the actual results after we run the experiment}
\end{abstract}

\section{Introduction}
The very first paragraph of RFC 3170 states:

\begin{quotation}
\noindent
IP Multicast will play a prominent role on the Internet in the coming
years.  It is a requirement, not an option, if the Internet is going
to scale.  Multicast allows application developers to add more
functionality without significantly impacting the network.
\end{quotation}

There is a need for some experimental data to back this kind of
statements.  We concentrate of measuring the impact of {\it software
updates\/} on the network, because the proliferation of connected
devices will make software updates a very important target for
efficient use of the network, and because software updates are
easy to simulate realistically by measuring the impact of copying
a large file to a large number of nodes.

To provide evidence for the above statements, we compare the
following methods of providing updates:

\begin{itemize}
\item Traditional unicast using a TCP-based service: a number
of servers listen to TCP requests to send a copy of the software
update, and each client selects a server at random and requests
the update from it: this is the mechanism used by the vast
majority of current services.
\item Full multicast: a number of servers provide the software
update using multicast, and clients will obtain the updates by
joining a multicast group and waiting for the data to arrive.
\item Unicast using a UDP-based service: this is similar to the
TCP case, but uses datagrams instead of virtual circuits: this
mechanism is introduced because multicast is by necessity based
on datagrams--there is no feedback from receiver to sender--and
we want to help determine which differences may be caused by
unicast vs.\ multicast, and which ones by virtual circuits vs\.
datagrams.
\end{itemize}

Independently of the scenario selected, there are two ``client''
scheduling strategies:

\begin{itemize}
\item All clients request updates at approximately the same time
(the ``immediate'' strategy).
\item Clients wait a random time before requesting the update
(the ``random'' strategy).
\end{itemize}

We run the simulated software updates in a variety of network
configurations and with a variety of file sizes to simulate the
impact of different types of updates; in each case we measure network
use, server load, client load and speed of update for each combination
of update mechanism and scheduling strategy.

The rest of this report is structured as follows:

Section~\ref{LAN:experiment} describes the simplest possible network
topology in which we can get useful measurements, and provides details
on how we run the experiment.

Sections~\ref{TWOLAN:experiment} and~\ref{GEN:experiment} describe
two more network topologies, incresing the complexity and studying
how different features affect the results.

Section~\ref{results} analyses the result of the experiment and compares
the efficiency of unicast, multicast, and the transitional technology.

Section~\ref{future} explores the possibility of further experiments,
to make the simulation more realistic and more complete.

Appendix~\ref{programs} provides some more details about the various
programs which ran as part of the experiment, and where to find the
full sources of all these programs.

\section{The ``LAN'' experiment}
\label{LAN:experiment}

A number of servers (denoted by $\cal S$) provide software updates for
${\cal C}$ local clients; these servers and client all share a LAN
so that updates have the shortest possible netowrk path.  This
experiment will allow us to compare multicast and unicast in the
simplest possible setting, and one which is possible on any existing
local network in which IPv6 is enabled, and could represent for example
distributing updates within an organisation.

The third experimental parameters indicates the size of the software updates
as the number ${\cal N}$ of bytes contained in it.  In the real world,
clients may be more or less up-to-date so that each one may request a
subset of all updates available; for this experiment we assume that
all clients have all previous updates and are just requesting the latest
one; a future experiment may consider some more complex ``real life''
scenarios.

Figure~\pref{s2:c8}
\begin{figure}[bp]
\begin{center}
\begin{picture}(90,29)
\multiput(25,22)(40,0){2}{\makebox(0,0){\rm\bf S}}
\multiput(25,16)(40,0){2}{\line(0,1){3}}
\put(10,16){\line(1,0){70}}
\multiput(10,13)(10,0){8}{\line(0,1){3}}
\multiput(10,10)(10,0){8}{\makebox(0,0){\rm\bf C}}
\end{picture}
\end{center}
\hspace*{\fill}%
\mbox{{\bf S} = server;}%
\hspace*{\fill}%
\mbox{{\bf C} = client}%
\hspace*{\fill}
\caption{Network with 2 servers and 8 clients on a single LAN}
\label{s2:c8}
\end{figure}
shows the network topology with ${\cal S} = 3$ and ${\cal C} = 8$, so
that there are 3 servers sending data to 8 clients on a single LAN.

\subsection{Experiment procedure}
For a specified network topology (i.e.\ values for ${\cal S}$ and ${\cal C}$),
and a list of update sizes (several distinct values for ${\cal N}$),
we implement that network on an experiment testbed, then run a series of
tests on it using all possible combinations of update size, update method
and scheduling strategy.  For organisational reasons, the testbed actually
has ${\cal S} + 1$ servers, the extra server does not take part in the
update but directs the operations and collects results (we call this extra
server the ``director'').

Each test starts with the director generating a file of the specified size
filled with random data; this is copied to all servers, so they all provide
an identical update.  Then the director waits 60 seconds to make sure that
the 1-minute load average of each node in the system is down to its
baseline value; when we ran experiments without this wait, we had each
run affecting the measurements of the next one, so it did not produce
useful results.

After the 60 seconds wait, the director asks all server nodes to start:
this means that two daemons start on each server, a resource monitoring
tool and the update provider appropriate for the selected update method.
These update providers are described below.

After all servers have started, the director asks all client nodes to
start as well: each client will first wait a short time depending on
the scheduling strategy: a fixed 4 seconds for ``immediate'' or a
random duration between 4 and 36 seconds for ``random''; the minimum
wait of 4 seconds is to make sure the rest of the system is fully
ready to run. After this delay, each client starts a resource monitoring
daemon identical to the one running on each server, and a client
program to obtain the update.

When a client has successfully obtained the update, the monitoring
daemon will record the time it has taken, finish another round of
resource measurements, and sends all the data back to the director.

The director waits for all clients to have sent the data, then asks
the servers to stop, at which point the servers will also send all
their resource measurements back to the director: all these measurements
are collected and copied to an external server for analysis.

The resource monitoring daemon records the following data every
second:

\begin{itemize}
\item 1-minute load average as provided by the system
\item memory and swap use
\item bytes sent and received on the network interface used to
transfer the update data
\end{itemize}

Additionally, at the end of the experiment it also records the following
data about the update program itself (update provider for servers, or
the program obtaining the update for clients):

\begin{itemize}
\item Time elapsed between start and termination of the program, in milliseconds
\item CPU time used by the program itself, in milliseconds
\item CPU time used by the operating system to run the program, in milliseconds
(this includes, for example, time used to obtain data from disk)
\item The termination status: whether the program reported an error
\end{itemize}

Since the experiment procedure is automated by running a single program
on the director, where possible we ran it many times on the same
testbed, to have more experimental data without the extra overhead
of setting up a new testbed.

\subsection{Update methods}
\TODO{describe the three update methods in more detail and update
for the methods we are actually using}

For the ``unicast'' experiment, each client selects a server at random,
connects to that server, downloads the file, calculates a checksum
to verify file integrity, and repeat if there is a problem.

For the ``multicast'' experiment, each server sends a copy of the
file on a multicast group, then repeat until all clients have received
it (this is known to the server because everybody will have
left the multicast group at this point).  Each client will
join the group, wait until it has all the required data and
the checksum is valid, then leave the group and stop.

\section{The ``Two LANs'' experiment}
\label{TWOLAN:experiment}

Similar to the previous experiment, but we investigate the effect of
a multicast router in the network: there are two LANs connected together
by a single router; all the servers are on the first LAN, and all the
clients are on the second LAN.  Like the previous experiments, the
parameters are the number of servers $\cal S$), the number of clients
${\cal C}$ clients, and the update size in bytes ${\cal N}$.

Figure~\pref{s3:r1:l8}
\begin{figure}[bpt]
\begin{center}
\begin{picture}(90,41)
\multiput(20,34)(25,0){3}{\makebox(0,0){\rm\bf S}}
\multiput(20,28)(25,0){3}{\line(0,1){3}}
\put(20,28){\line(1,0){50}}
\put(45,25){\line(0,1){3}}
\put(45,22){\makebox(0,0){\rm\bf R}}
\put(45,16){\line(0,1){3}}
\put(10,16){\line(1,0){70}}
\multiput(10,13)(10,0){8}{\line(0,1){3}}
\multiput(10,10)(10,0){8}{\makebox(0,0){\rm\bf C}}
\end{picture}
\end{center}
\hspace*{\fill}%
\mbox{{\bf S} = server;}%
\hspace*{\fill}%
\mbox{{\bf R} = router;}%
\hspace*{\fill}%
\mbox{{\bf C} = client}%
\hspace*{\fill}
\caption{Two LANs network, with 3 servers and 8 clients}
\label{s3:r1:l8}
\end{figure}
shows the network topology with ${\cal S} = 3$ and ${\cal C} = 8$,
so that there are 3 servers sending data to 8 clients.

The experimental procedure is very similar to the previous experiment,
we only describe the differences between them here.

After copying the update data to all servers and waiting 60 seconds, the
director will ask the router to start its own resource monitoring, and,
for the multicast experiment, to start a multicast routing daemon.

The servers wait 2 seconds before start, to give the router time to be
fully set up.

After that, the experiment proceeds identically with the director starting
all clients and waiting for results.

\section{The ``Generic'' experiment}
\label{GEN:experiment}

A number of servers (denoted by $\cal S$) provide software updates for
${\cal C} = {\cal L} * 2^{{\cal R} - 1}$ clients; there are ${\cal R}$ routers
in the network path between each server and each client, and the clients
are connected to a LAN, at ${\cal L}$ clients on each local network.
For this experiments, all the servers share a LAN.
This is of course a simplified view of a real system but allows to
compare the impact of various methods of software updates in a variety
of circumstances.

The last experimental parameters indicates the size of the software updates
as the number ${\cal N}$ of bytes contained in it.  In the real world,
clients may be more or less up-to-date so that each one may request a
subset of all updates available; for this experiment we assume that
all clients have all previous updates and are just requesting the latest
one; a future experiment may consider some more complex ``real life''
scenarios.

Figure~\pref{s3:r3:l2}
\begin{figure}[bp]
\begin{center}
\begin{picture}(90,65)
\multiput(20,58)(25,0){3}{\makebox(0,0){\rm\bf S}}
\multiput(20,52)(25,0){3}{\line(0,1){3}}
\put(20,52){\line(1,0){50}}
\put(45,49){\line(0,1){3}}
\put(45,46){\makebox(0,0){\rm\bf R}}
\put(45,40){\line(0,1){3}}
\put(25,40){\line(1,0){40}}
\multiput(25,37)(40,0){2}{\line(0,1){3}}
\multiput(25,34)(40,0){2}{\makebox(0,0){\rm\bf R}}
\multiput(25,28)(40,0){2}{\line(0,1){3}}
\multiput(15,28)(40,0){2}{\line(1,0){20}}
\multiput(15,25)(20,0){4}{\line(0,1){3}}
\multiput(15,22)(20,0){4}{\makebox(0,0){\rm\bf R}}
\multiput(15,16)(20,0){4}{\line(0,1){3}}
\multiput(10,16)(20,0){4}{\line(1,0){10}}
\multiput(10,13)(10,0){8}{\line(0,1){3}}
\multiput(10,10)(10,0){8}{\makebox(0,0){\rm\bf C}}
\end{picture}
\end{center}
\hspace*{\fill}%
\mbox{{\bf S} = server;}%
\hspace*{\fill}%
\mbox{{\bf R} = router;}%
\hspace*{\fill}%
\mbox{{\bf C} = client}%
\hspace*{\fill}
\caption{Network with 3 servers and 8 clients, 2 clients per LAN}
\label{s3:r3:l2}
\end{figure}
shows the network topology with ${\cal S} = 3$, ${\cal R} = 3$ and
${\cal L} = 2$, so that there are 3 servers sending data to 8 clients
organised in 4 separate LANs;
for comparison, figure~\pref{s3:r4:l1}
\begin{figure}[bp]
\begin{center}
\begin{picture}(90,74)
\multiput(20,67)(25,0){3}{\makebox(0,0){\rm\bf S}}
\multiput(20,61)(25,0){3}{\line(0,1){3}}
\put(20,61){\line(1,0){50}}
\put(45,58){\line(0,1){3}}
\put(45,55){\makebox(0,0){\rm\bf R}}
\put(45,49){\line(0,1){3}}
\put(25,49){\line(1,0){40}}
\multiput(25,46)(40,0){2}{\line(0,1){3}}
\multiput(25,43)(40,0){2}{\makebox(0,0){\rm\bf R}}
\multiput(25,37)(40,0){2}{\line(0,1){3}}
\multiput(15,37)(40,0){2}{\line(1,0){20}}
\multiput(15,34)(20,0){4}{\line(0,1){3}}
\multiput(15,31)(20,0){4}{\makebox(0,0){\rm\bf R}}
\multiput(15,25)(20,0){4}{\line(0,1){3}}
\multiput(10,25)(20,0){4}{\line(1,0){10}}
\multiput(10,22)(10,0){8}{\line(0,1){3}}
\multiput(10,19)(10,0){8}{\makebox(0,0){\rm\bf R}}
\multiput(10,13)(10,0){8}{\line(0,1){3}}
\multiput(10,10)(10,0){8}{\makebox(0,0){\rm\bf C}}
\end{picture}
\end{center}
\hspace*{\fill}%
\mbox{{\bf S} = server;}%
\hspace*{\fill}%
\mbox{{\bf R} = router;}%
\hspace*{\fill}%
\mbox{{\bf C} = client}%
\hspace*{\fill}
\caption{Network with 3 servers and 8 clients, 1 client per LAN}
\label{s3:r4:l1}
\end{figure}
shows the same number of clients arranged on 8 separate LANs
(${\cal L} = 1$) so that there is an extra network hop between
clients and servers (${\cal R} = 4$).

Each experiment run will select a network topology (i.e.\ values
for ${\cal S}$, ${\cal R}$ and ${\cal L}$) and an update size
${\cal N}$, then run three times with the three update mechanisms.

We run each experiment until all clients have succeeded obtaining
a copy of the software update. During this time, various resource
usages are monitored to show the impact of an update mechanism on
servers, routers, network and clients. Additionally, the time
each client takes to obtain the software update is measured, reporting
the minimum, average and maximum time experienced by the clients.

For the ``unicast'' experiment, all routers are just normal
unicast routers able to send packets from the clients to
the server and back.  Each client selects a server at random,
connects to that server, downloads the file, calculates a
checksum to verify file integrity, and repeat if there
is a problem.

For the ``multicast'' experiment, all routers support
multicast routing so when a client joins a multicast group
the routers will make sure packets get from the servers to
that client.  Each server sends a copy of the file on a
multicast group, then repeat until all clients have received
it (this is known to the server because everybody will have
left the multicast group at this point).  Each client will
join the group, wait until it has all the required data and
the checksum is valid, then leave the group and stop.

The ``hybrid'' experiment simulates the transitional technology
offered by librecast. Servers and clients will behave exactly
in the same way as the ``multicast'' experiment; however the
routers directly connected to servers and the ones directly
connected to clients will implement the transitional technology,
while the intermediate routers do not support multicast routing.

\section{Experiment results}
\label{results}

\section{Future work}
\label{future}

Due to time limitations we have only measured network performance
for a small set of regular network topologies generated by varying
some parameters; of course the real world is made up of rather
more irregular topologies and it it would be interesting to investigate
more variations in this area in a future set of experiments.

We also simplified the software update by assuming that all clients
request exactly the same file, rather than a more complex situation
in which every client requests a different subset of all available
updates, due to its own unique update history; while we expect that
multicast will scale really well to this situation, we haven't
ran an experiment to support this.

Another type of network activity which can benefit from multicast is
live streaming, where the server will only need to send the stream once;
this case is similar to software updates and probably does not need a
separate experiment; however if several choices of bandwidth and quality
are required the situation is different.  In the unicast case it's obvious
how the sender can provide different quality streams to different clients,
for multicast the simplest answer is to provide several streams with
different quality, with the client subscribing to the one which best
match its requirements: this would save network and server resources
but there may be better way of achieving this result, for example using
layered codecs to send only one copy of the lowest quality stream, then
a second stream with the difference between that and the next highest
quality. We don't know at present if these codecs would involve more
server resources than the re-encoding required to provide multiple
stream with different quality, but in any case we would find it useful
to run another experiment to measure these costs and compare them with
the expected savings in terms of network usage.

\appendix
\section{Programs}
\label{programs}

To run each experiment we had to implement a network topology on an
experiment testbed, set up each node in the testbed, run the experiment
itself and collect the results; additionally, we had to analyse the
results of groups of experiments together.  This appendix describes
the programs used for all various tasks, and includes references to
where the full source code can be found for the programs we developed.

\subsection{Preparing a testbed}

{\em The programs described here and other useful tools are in the experiment
setup repository~\cite{exp:scripts} under the {\tt bin} directory.}

Given the number of servers, clients and routers (if appropriate to the
experiment) we developed a simple program to generate action files
for ``jfed'' so that the process could be automated; a single program
``mknet'' provided action files for all the experiment topologies
described in this report by providing appropriate options; for the
networks shown as examples in the figures we just ran:

\begin{verbatim}
mknet S=3 L=8
mknet S=3 R=1 L=8
mknet S=3 R=2 L=2
mknet S=3 R=3 L=1
\end{verbatim}

As can be seen, omitting ``R'' results in a single LAN network in which
the number of clients is specified by ``L'' for consistency with the
other networks (where it indicates the number of clients on each client
LAN).

By default, the program generates an experiment name indicating the parameters
provided: for the four examples above this would be: ``S3L8'', ``S3R1L8'',
``S3R2L2'' and ``S3R3L1''; the data generated will be stored in a
directory inside {\tt/tmp} named after the experiment.

One of the files generated, {\tt action.json}, is suitable for using as
an action file with the ``jfed-cli'' tool and will provision the testbed;
the program also generates {\tt action.rspec} which is suitable for
using with the ``jfed-gui'' tool.  We do not describe these tools here
as they are provided by fed4fire.

Once the testbed is up and running, we need to copy some things to it,
for example the actual programs which will run on it and information about
the experiment to run.  The list of data sizes is also specified at
this point, for example to run with 32, 64 and 512 megabytes on the
first single LAN experiment defined above (``S3L8'')

\begin{verbatim}
setup-experiment 32,64,512 S3L8
\end{verbatim}

This sets up the ``director'' node.  To complete the setup, one needs to
connect to it and then run a program there:

\begin{verbatim}
ssh-experiment S3L8 director0
/tmp/experiment/setup-all
\end{verbatim}

The testbed is now ready to run the experiment as many times as one wants.

\subsection{Running an experiment}
\TODO{ssh-experiment, setup-all, run-experiment}

\subsubsection{Resource monitoring}
\TODO{lwmon}

\subsubsection{Update programs}
\TODO{iotupd, iotup, \ldots}

\subsection{Importing results in a database}
\TODO{extract-data, import-into-sqlite}

\subsection{Analysing results}
\TODO{extract-result, \ldots}

\section*{References}
\bibliographystyle{plain}
\begin{thebibliography}{99}

\bibitem{exp:scripts}
  C. Calvelli, E. Payne, B. Sheffield.
  {\em Fed4fire experiment setup scripts}.
  git: TODO

\end{thebibliography}

\end{document}


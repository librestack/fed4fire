\documentclass[a4paper,12pt]{article}

\usepackage{ifpdf}

\title{Comparing performance of IPv6 multicast and unicast for software updates}
\author{Claudio Calvelli}
\date{Draft 0.3 \today}

\setlength{\unitlength}{0.01\linewidth}

\newcommand{\TODO}[1]{\par\noindent%
\hspace*{\fill}%
\framebox{\parbox{0.9\linewidth}{{\bf TODO: }#1}}%
\hspace*{\fill}%
}

% everybody say to just include the hyperref package - it's big, ugly and
% breaks the next command.  This on the other hand works just fine
\ifpdf
\newcommand{\url}[1]{%
\pdfstartlink %
attr {/Border [0 0 0]} %
user {/Subtype /Link /A << /S /URI /URI (#1) >>}%
{\tt #1}\pdfendlink}
\else
\newcommand{\url}[1]{{\tt #1}}
\fi

\newcommand{\pref}[1]{%
\ref{#1}%
\ifnum\thepage=0\pageref{#1}\else\ on page~\pageref{#1}\fi%
}

\begin{document}
\maketitle

\begin{abstract}
The librecast project states that ``Multicast is, by definition, the most
efficient way for multiple nodes to communicate''.  This experiment is
designed to provide evidence of this efficiency by comparing three
methods of sending the same data to a large number of nodes, as
would for example happen when a software update is released. The three
methods are: traditional unicast; a network taking full advantage of
multicast; and finally a hybrid designed to represent the use of transitional
technology to address the lack of multicast routing in many networks.

\TODO{Report the actual results after we run the experiment}
\end{abstract}

\section{Introduction}
The very first paragraph of RFC 3170 states:

\begin{quotation}
\noindent
IP Multicast will play a prominent role on the Internet in the coming
years.  It is a requirement, not an option, if the Internet is going
to scale.  Multicast allows application developers to add more
functionality without significantly impacting the network.
\end{quotation}

There is a need for some experimental data to back these statements.  We concentrate of measuring the impact of {\it software
updates\/} on the network, because the proliferation of connected
devices will make software updates a very important target for
efficient use of the network, and because software updates are
easy to simulate realistically by measuring the impact of copying
a large file to a large number of nodes.

To provide evidence for the above statements, we compare the
following methods of providing updates:

\begin{itemize}
\item Traditional unicast using a TCP-based service: a number
of servers listen to TCP requests to send a copy of the software
update, and each client selects a server at random and requests
the update from it: this is the mechanism used by the vast
majority of current services.
\item Full multicast: a number of servers provide the software
update using multicast, and clients will obtain the updates by
joining a multicast group and waiting for the data to arrive.
\item Unicast using a UDP-based service: this is similar to the
TCP case, but uses datagrams instead of virtual circuits: this
mechanism is introduced because multicast is by necessity based
on datagrams--there is no feedback from receiver to sender--and
we want to help determine which differences may be caused by
unicast vs.\ multicast, and which ones by virtual circuits vs\.
datagrams.
\end{itemize}

Independently of the scenario selected, there are two ``client''
scheduling strategies:

\begin{itemize}
\item All clients request updates at approximately the same time
(the ``immediate'' strategy).
\item Clients wait a random time before requesting the update
(the ``random'' strategy).
\end{itemize}

We run the simulated software updates in a variety of network
configurations and with a variety of file sizes to simulate the
impact of different types of updates; in each case we measure network
use, server load, client load and speed of update for each combination
of update mechanism and scheduling strategy.

The rest of this report is structured as follows:

Section~\ref{LAN:experiment} describes the simplest possible network
topology in which we can get useful measurements, and provides details
on how we run the experiment.

Sections~\ref{TWOLAN:experiment} and~\ref{GEN:experiment} describe
two more network topologies, incresing the complexity and studying
how different features affect the results.

Section~\ref{results} analyses the result of the experiment and compares
the efficiency of unicast, multicast, and the transitional technology.

Section~\ref{future} explores the possibility of further experiments,
to make the simulation more realistic and more complete.

Appendix~\ref{programs} provides some more details about the various
programs which ran as part of the experiment, and where to find the
full sources of all these programs.

\section{The ``LAN'' experiment}
\label{LAN:experiment}

A number of servers (denoted by $\cal S$) provide software updates for
${\cal C}$ local clients; these servers and client all share a LAN
so that updates have the shortest possible netowrk path.  This
experiment will allow us to compare multicast and unicast in the
simplest possible setting, and one which is possible on any existing
local network in which IPv6 is enabled, and could represent for example
distributing updates within an organisation.

The third experimental parameters indicates the size of the software updates
as the number ${\cal N}$ of bytes contained in it.  In the real world,
clients may be more or less up-to-date so that each one may request a
subset of all updates available; for this experiment we assume that
all clients have all previous updates and are just requesting the latest
one; a future experiment may consider some more complex ``real life''
scenarios.

Figure~\pref{s2:c8}
\begin{figure}[bp]
\begin{center}
\begin{picture}(90,29)
\multiput(25,22)(40,0){2}{\makebox(0,0){\rm\bf S}}
\multiput(25,16)(40,0){2}{\line(0,1){3}}
\put(10,16){\line(1,0){70}}
\multiput(10,13)(10,0){8}{\line(0,1){3}}
\multiput(10,10)(10,0){8}{\makebox(0,0){\rm\bf C}}
\end{picture}
\end{center}
\hspace*{\fill}%
\mbox{{\bf S} = server;}%
\hspace*{\fill}%
\mbox{{\bf C} = client}%
\hspace*{\fill}
\caption{Network with 2 servers and 8 clients on a single LAN}
\label{s2:c8}
\end{figure}
shows the network topology with ${\cal S} = 3$ and ${\cal C} = 8$, so
that there are 3 servers sending data to 8 clients on a single LAN.

\subsection{Experiment procedure}
For a specified network topology (i.e.\ values for ${\cal S}$ and ${\cal C}$),
and a list of update sizes (several distinct values for ${\cal N}$),
we implement that network on an experiment testbed, then run a series of
tests on it using all possible combinations of update size, update method
and scheduling strategy.  For organisational reasons, the testbed actually
has ${\cal S} + 1$ servers, the extra server does not take part in the
update but directs the operations and collects results (we call this extra
server the ``director'').

Each test starts with the director generating a file of the specified size
filled with random data; this is copied to all servers, so they all provide
an identical update.  Then the director waits 60 seconds to make sure that
the 1-minute load average of each node in the system is down to its
baseline value; when we ran experiments without this wait, we had each
run affecting the measurements of the next one, so it did not produce
useful results.

After the 60 seconds wait, the director asks all server nodes to start:
this means that two daemons start on each server, a resource monitoring
tool and the update provider appropriate for the selected update method.
These update providers are described below.

After all servers have started, the director asks all client nodes to
start as well: each client will first wait a short time depending on
the scheduling strategy: a fixed 4 seconds for ``immediate'' or a
random duration between 4 and 36 seconds for ``random''; the minimum
wait of 4 seconds is to make sure the rest of the system is fully
ready to run. After this delay, each client starts a resource monitoring
daemon identical to the one running on each server, and a client
program to obtain the update.

When a client has successfully obtained the update, the monitoring
daemon will record the time it has taken, finish another round of
resource measurements, and sends all the data back to the director.

The director waits for all clients to have sent the data, then asks
the servers to stop, at which point the servers will also send all
their resource measurements back to the director: all these measurements
are collected into a single ``tar'' archive and copied to an external
server for later analysis.

The resource monitoring daemon records the following data every
second:

\begin{itemize}
\item 1-minute load average as provided by the system
\item memory and swap use
\item bytes sent and received on the network interface used to
transfer the update data
\end{itemize}

Additionally, at the end of the experiment it also records the following
data about the update program itself (update provider for servers, or
the program obtaining the update for clients):

\begin{itemize}
\item Time elapsed between start and termination of the program, in milliseconds
\item CPU time used by the program itself, in milliseconds
\item CPU time used by the operating system to run the program, in milliseconds
(this includes, for example, time used to obtain data from disk)
\item The termination status: whether the program reported an error
\end{itemize}

Since the experiment procedure is automated by running a single program
on the director, where possible we ran it many times on the same
testbed, to have more experimental data without the extra overhead
of setting up a new testbed.

\subsection{Update methods}
\TODO{describe the three update methods in more detail and update
for the methods we are actually using}

For the ``unicast'' experiment, each client selects a server at random,
connects to that server, downloads the file, calculates a checksum
to verify file integrity, and repeat if there is a problem.

For the ``multicast'' experiment, each server sends a copy of the
file on a multicast group, then repeat until all clients have received
it (this is known to the server because everybody will have
left the multicast group at this point).  Each client will
join the group, wait until it has all the required data and
the checksum is valid, then leave the group and stop.

\section{The ``Two LANs'' experiment}
\label{TWOLAN:experiment}

Similar to the previous experiment, but we investigate the effect of
a multicast router in the network: there are two LANs connected together
by a single router; all the servers are on the first LAN, and all the
clients are on the second LAN.  Like the previous experiments, the
parameters are the number of servers $\cal S$), the number of clients
${\cal C}$ clients, and the update size in bytes ${\cal N}$.

Figure~\pref{s3:r1:l8}
\begin{figure}[bpt]
\begin{center}
\begin{picture}(90,41)
\multiput(20,34)(25,0){3}{\makebox(0,0){\rm\bf S}}
\multiput(20,28)(25,0){3}{\line(0,1){3}}
\put(20,28){\line(1,0){50}}
\put(45,25){\line(0,1){3}}
\put(45,22){\makebox(0,0){\rm\bf R}}
\put(45,16){\line(0,1){3}}
\put(10,16){\line(1,0){70}}
\multiput(10,13)(10,0){8}{\line(0,1){3}}
\multiput(10,10)(10,0){8}{\makebox(0,0){\rm\bf C}}
\end{picture}
\end{center}
\hspace*{\fill}%
\mbox{{\bf S} = server;}%
\hspace*{\fill}%
\mbox{{\bf R} = router;}%
\hspace*{\fill}%
\mbox{{\bf C} = client}%
\hspace*{\fill}
\caption{Two LANs network, with 3 servers and 8 clients}
\label{s3:r1:l8}
\end{figure}
shows the network topology with ${\cal S} = 3$ and ${\cal C} = 8$,
so that there are 3 servers sending data to 8 clients.

The experimental procedure is very similar to the previous experiment,
we only describe the differences between them here.

After copying the update data to all servers and waiting 60 seconds, the
director will ask the router to start its own resource monitoring, and,
for the multicast experiment, to start a multicast routing daemon.

The servers wait 2 seconds before start, to give the router time to be
fully set up.

After that, the experiment proceeds identically with the director starting
all clients and waiting for results.

\section{The ``Generic'' experiment}
\label{GEN:experiment}

A number of servers (denoted by $\cal S$) provide software updates for
${\cal C} = {\cal L} * 2^{{\cal R} - 1}$ clients; there are ${\cal R}$ routers
in the network path between each server and each client, and the clients
are connected to a LAN, at ${\cal L}$ clients on each local network.
For this experiments, all the servers share a LAN.
This is of course a simplified view of a real system but allows to
compare the impact of various methods of software updates in a variety
of circumstances.

The last experimental parameters indicates the size of the software updates
as the number ${\cal N}$ of bytes contained in it.  In the real world,
clients may be more or less up-to-date so that each one may request a
subset of all updates available; for this experiment we assume that
all clients have all previous updates and are just requesting the latest
one; a future experiment may consider some more complex ``real life''
scenarios.

Figure~\pref{s3:r3:l2}
\begin{figure}[bp]
\begin{center}
\begin{picture}(90,65)
\multiput(20,58)(25,0){3}{\makebox(0,0){\rm\bf S}}
\multiput(20,52)(25,0){3}{\line(0,1){3}}
\put(20,52){\line(1,0){50}}
\put(45,49){\line(0,1){3}}
\put(45,46){\makebox(0,0){\rm\bf R}}
\put(45,40){\line(0,1){3}}
\put(25,40){\line(1,0){40}}
\multiput(25,37)(40,0){2}{\line(0,1){3}}
\multiput(25,34)(40,0){2}{\makebox(0,0){\rm\bf R}}
\multiput(25,28)(40,0){2}{\line(0,1){3}}
\multiput(15,28)(40,0){2}{\line(1,0){20}}
\multiput(15,25)(20,0){4}{\line(0,1){3}}
\multiput(15,22)(20,0){4}{\makebox(0,0){\rm\bf R}}
\multiput(15,16)(20,0){4}{\line(0,1){3}}
\multiput(10,16)(20,0){4}{\line(1,0){10}}
\multiput(10,13)(10,0){8}{\line(0,1){3}}
\multiput(10,10)(10,0){8}{\makebox(0,0){\rm\bf C}}
\end{picture}
\end{center}
\hspace*{\fill}%
\mbox{{\bf S} = server;}%
\hspace*{\fill}%
\mbox{{\bf R} = router;}%
\hspace*{\fill}%
\mbox{{\bf C} = client}%
\hspace*{\fill}
\caption{Network with 3 servers and 8 clients, 2 clients per LAN}
\label{s3:r3:l2}
\end{figure}
shows the network topology with ${\cal S} = 3$, ${\cal R} = 3$ and
${\cal L} = 2$, so that there are 3 servers sending data to 8 clients
organised in 4 separate LANs;
for comparison, figure~\pref{s3:r4:l1}
\begin{figure}[bp]
\begin{center}
\begin{picture}(90,74)
\multiput(20,67)(25,0){3}{\makebox(0,0){\rm\bf S}}
\multiput(20,61)(25,0){3}{\line(0,1){3}}
\put(20,61){\line(1,0){50}}
\put(45,58){\line(0,1){3}}
\put(45,55){\makebox(0,0){\rm\bf R}}
\put(45,49){\line(0,1){3}}
\put(25,49){\line(1,0){40}}
\multiput(25,46)(40,0){2}{\line(0,1){3}}
\multiput(25,43)(40,0){2}{\makebox(0,0){\rm\bf R}}
\multiput(25,37)(40,0){2}{\line(0,1){3}}
\multiput(15,37)(40,0){2}{\line(1,0){20}}
\multiput(15,34)(20,0){4}{\line(0,1){3}}
\multiput(15,31)(20,0){4}{\makebox(0,0){\rm\bf R}}
\multiput(15,25)(20,0){4}{\line(0,1){3}}
\multiput(10,25)(20,0){4}{\line(1,0){10}}
\multiput(10,22)(10,0){8}{\line(0,1){3}}
\multiput(10,19)(10,0){8}{\makebox(0,0){\rm\bf R}}
\multiput(10,13)(10,0){8}{\line(0,1){3}}
\multiput(10,10)(10,0){8}{\makebox(0,0){\rm\bf C}}
\end{picture}
\end{center}
\hspace*{\fill}%
\mbox{{\bf S} = server;}%
\hspace*{\fill}%
\mbox{{\bf R} = router;}%
\hspace*{\fill}%
\mbox{{\bf C} = client}%
\hspace*{\fill}
\caption{Network with 3 servers and 8 clients, 1 client per LAN}
\label{s3:r4:l1}
\end{figure}
shows the same number of clients arranged on 8 separate LANs
(${\cal L} = 1$) so that there is an extra network hop between
clients and servers (${\cal R} = 4$).

Each experiment run will select a network topology (i.e.\ values
for ${\cal S}$, ${\cal R}$ and ${\cal L}$) and an update size
${\cal N}$, then run three times with the three update mechanisms.

We run each experiment until all clients have succeeded obtaining
a copy of the software update. During this time, various resource
usages are monitored to show the impact of an update mechanism on
servers, routers, network and clients. Additionally, the time
each client takes to obtain the software update is measured, reporting
the minimum, average and maximum time experienced by the clients.

For the ``unicast'' experiment, all routers are just normal
unicast routers able to send packets from the clients to
the server and back.  Each client selects a server at random,
connects to that server, downloads the file, calculates a
checksum to verify file integrity, and repeat if there
is a problem.

For the ``multicast'' experiment, all routers support
multicast routing so when a client joins a multicast group
the routers will make sure packets get from the servers to
that client.  Each server sends a copy of the file on a
multicast group, then repeat until all clients have received
it (this is known to the server because everybody will have
left the multicast group at this point).  Each client will
join the group, wait until it has all the required data and
the checksum is valid, then leave the group and stop.

The ``hybrid'' experiment simulates the transitional technology
offered by librecast. Servers and clients will behave exactly
in the same way as the ``multicast'' experiment; however the
routers directly connected to servers and the ones directly
connected to clients will implement the transitional technology,
while the intermediate routers do not support multicast routing.

\section{Experiment results}
\label{results}

\section{Future work}
\label{future}

Due to time limitations we have only measured network performance
for a small set of regular network topologies generated by varying
some parameters; of course the real world is made up of rather
more irregular topologies and it it would be interesting to investigate
more variations in this area in a future set of experiments.

We also simplified the software update by assuming that all clients
request exactly the same file, rather than a more complex situation
in which every client requests a different subset of all available
updates, due to its own unique update history; while we expect that
multicast will scale really well to this situation, we haven't
ran an experiment to support this.

Another type of network activity which can benefit from multicast is
live streaming, where the server will only need to send the stream once;
this case is similar to software updates and probably does not need a
separate experiment; however if several choices of bandwidth and quality
are required the situation is different.  In the unicast case it's obvious
how the sender can provide different quality streams to different clients,
for multicast the simplest answer is to provide several streams with
different quality, with the client subscribing to the one which best
match its requirements: this would save network and server resources
but there may be better way of achieving this result, for example using
layered codecs to send only one copy of the lowest quality stream, then
a second stream with the difference between that and the next highest
quality. We don't know at present if these codecs would involve more
server resources than the re-encoding required to provide multiple
stream with different quality, but in any case we would find it useful
to run another experiment to measure these costs and compare them with
the expected savings in terms of network usage.

\appendix
\section{Programs}
\label{programs}

To run each experiment we had to implement a network topology on an
experiment testbed, set up each node in the testbed, run the experiment
itself and collect the results; additionally, we had to analyse the
results of groups of experiments together.  This appendix describes
the programs used for all various tasks, and includes references to
where the full source code can be found for the programs we developed.

\subsection{Preparing a testbed and running an experiment}

{\em The programs described here and other useful tools are in the experiment
setup repository~\cite{exp:scripts} under the {\tt bin} directory for
the programs to run on the local system and the {\tt objects} directory
for the programs to run on the testbed.}

Given the number of servers, clients and routers (if appropriate to the
experiment) we developed a simple program to generate action files
for ``jfed''~\cite{jfed} so that the process could be automated; a single program
``mknet'' provided action files for all the experiment topologies
described in this report by providing appropriate options; for the
networks shown as examples in the figures we just ran:

\begin{verbatim}
local$ mknet S=3 L=8
local$ mknet S=3 R=1 L=8
local$ mknet S=3 R=2 L=2
local$ mknet S=3 R=3 L=1
\end{verbatim}

As can be seen, omitting ``R'' results in a single LAN network in which
the number of clients is specified by ``L'' for consistency with the
other networks (where it indicates the number of clients on each client
LAN).

By default, the program generates an experiment name indicating the parameters
provided: for the four examples above this would be: ``S3L8'', ``S3R1L8'',
``S3R2L2'' and ``S3R3L1''; the data generated will be stored in a
directory inside {\tt/tmp} named after the experiment.

One of the files generated, {\tt action.yaml}, is suitable for using as
an action file with the ``jfed-cli'' tool and will provision the testbed;
the program also generates {\tt action.rspec} which is suitable for
using with the ``jfed-gui'' tool.  We do not describe these tools here
as they are provided by fed4fire, but see~\cite{jfed}.

Once the testbed is up and running, we need to copy some things to it,
for example the actual programs which will run on it and information about
the experiment to run.  The list of data sizes is also specified at
this point, for example to run with 32, 64 and 512 megabytes on the
first single LAN experiment defined above (``S3L8'')

\begin{verbatim}
local$ setup-experiment 32,64,512 S3L8
\end{verbatim}

This sets up the ``director'' node and copies the {\tt objects} directory
of the repository to it.  To complete the setup, one needs to connect to
it and then run a program there:

\begin{verbatim}
local$ ssh-experiment S3L8 director0
director0$ /tmp/experiment/setup-all
\end{verbatim}

The testbed is now ready to run the experiment by running a program on
director0:

\begin{verbatim}
local$ ssh-experiment S3L8 director0
director0$ /tmp/experiment/run-experiment
\end{verbatim}

If required, the ``{\tt run-experiment}'' program can run many times
to obtain more data without additional setup overhead; no need to
repeat any of the previous steps.

Internally, the ``{\tt run-experiment}'' program calls other programs
running on the director, but also on servers, routers and clients
as necessary to implement the procedure described in
sections~\ref{LAN:experiment} to~\ref{GEN:experiment}. These
have names like ``{\tt start-tcp-server}'' or
``{\tt start-multicast-router}'' to start wat is required for
a particular experiment on a particular node (in this example,
start the TCP experiment on a server, and start the multicast
routing daemon on a router, respectively).  All these programs are
found in the repository cited.

One issue we found while developing programs to automate the experiment
is that network interface names may be different when booting
different testbeds; each node has two interfaces, a control
interface used by the testbed administration, as well as to log
in to it from outside the experiment, and a second interface
connected as required by the experiment's network topology and
used to transfer the data files during the experiment; for a router
node, there are obviously more interfaces.  We did not have time
to develop our own boot image to work around the problem that
systemd's ``predictable'' interface names turn out to be nothing
of the sort, so the ``setup-all'' script will additionally have
to figure out what interface is going to be used for what, and
configure them as required: this may be different for each node.
It will also assign friendlier names to the interface which tell
us what is used for what, as is important when looking at network
usage data for routers.

\subsubsection{Resource monitoring}
{\em The program described here is in the lwmon repository~\cite{lwmon}.}

There are many monitoring tools for Unix system, however in our experience
they tend to use more resources than programs which do the actual work,
or else they are designed to sample information only once a minute,
which is not enough for this experiment.

Because of experience using other monitoring tools and not finding
one which we actually want to use on a live system, we have developed
our own over the years, which we call ``lwmon'', for Light-Weight
system MONitoring, which, as the name suggests, is very considerate
in its use of resources and can safely run very frequently without
impact on the system.  Also, after it completes its initialisation,
it has constant memory usage, unlike other tools which can fail
in the middle of processing when they try to allocate more memory
on a busy system.  It can also report on its own resource usage,
so one can confirm that it is, indeed, light-weight.

Without going into complete details, each node sets up its own
configuration for lwmon, which then monitors memory usage and
system load every second, its own resource usage every 10
seconds.  The appropriate program (update provider for servers,
routing daemon for routers if requires by the experiment, and
the program which gets the updates for clients) also runs as
a child process of lwmon, so the latter can report on the resource
used by this program.

A lwmon configuration for a client looks like:

\begin{verbatim}
hostname client2

load lavg 1
memory memswap 1
network if1 1 enp6s0
self self 2

program exp 5 /tmp/experiment/start-multicast-client enp6s0
print /tmp/results/client2,multicast,random.64 overwrite binary
\end{verbatim}

This means that load average, memory/swap usage and network usage are
sampled every second, lwmon's own resource usage every 2 seconds, will
run the appropriate program for a particular experiment (in this case,
update via multicast), and save the information into a file in a compact
binary format. The second column of the lines specifying what to measure
is the name used to report it.

The program running does not depend on the scheduling strategy selected,
as that is handled before: we do not want to measure the time it takes
to sleep for a random duration, only the time it takes to download the
update.  The file name for the results, on the other hand, contain the
scheduling strategy (in this case, random) and the update size (64
megabytes), as we need to keep things distinct.

Another thing to note is that the ``network'' line asks to monitor
the interface using its systemd name (enp6s0 in this example) because
that's how it will be able to find it in the system, but reports it
using the name ``if1'' as this is the interface name we have used
in the action file and rspec.

Configuration for a server is essentially identical, with ``client''
replaced by ``server'', and for a router is very similar:

\begin{verbatim}
hostname router0

load lavg 1
memory memswap 1
network if1 1 enp4s0
network if2 1 enp6s0
self self 2

program exp 5 /tmp/experiment/start-multicast-router enp4s0
print /tmp/results/router0,multicast,random.64 overwrite binary
\end{verbatim}

The network interface reported as ``if1'' has network traffic going
to or from clients, and ``if2'' has traffic going to or from servers.

Once the ``{\tt start-}\ldots'' program terminates, lwmon automatically
reports on its resource usage and the wall clock time it has taken to run,
then runs one more round of measurements and exits.  For clients, the
program normally terminates when it has obtained the update successfully;
for servers and routers the program terminates when the director signals
the end of the experiment.

The program which generates the lwmon configuration file and calls lwmon
will wait for it to terminate, then copies the file it produced back
to the director node. This allows the director to collect all the
data about the experiment in one place.

As mentioned, lwmon produces a file containing data in a packed binary
format. To just look at the data, the tool has an option to read that
binary file back in and produce a human-readable output:

\begin{verbatim}
$ lwmon -R /tmp/results/router0,multicast,random.64 -P -
\end{verbatim}

There is a separate tool which reads one or more data files and produces
SQL statements which can be used to import it into a database, for example:

\begin{verbatim}
$ lwmon-to-sqlC [options] DATA_FILES | sqlite3 database.sqlite
\end{verbatim}

As described below in~\ref{importing}, for this experiment we have developed
a tool which calls {\tt lwmon-to-sql} in the appropriate way to import
experiment results in a format suitable for further analysis.

\subsubsection{Programs implementing the update methods}
\TODO{iotupd, iotup, \ldots}

\subsection{Importing results in a database}
\label{importing}
\TODO{extract-data, import-into-sqlite}

\subsection{Analysing results}
\TODO{extract-result, \ldots}

\section*{References}
\bibliographystyle{plain}
\begin{thebibliography}{99}
\begin{sloppypar}

\bibitem{exp:scripts}
  C. Calvelli, E. Payne, B. Sheffield.
  {\em Fed4fire experiment setup scripts}.\\
  \url{https://example.com/TODO-need-url}

\bibitem{lwmon}
  C. Calvelli.
  {\em lwmon: A {\bf\em l}ight-{\bf\em w}eight system {\bf\em mon}itoring tool}.\\
  \url{https://example.com/TODO-need-url}

\bibitem{jfed}
  {\em jfed 5.9 documentation}.\\
  \url{https://doc.ilabt.imec.be/jfed-documentation-5.9/index.html}

\end{sloppypar}
\end{thebibliography}

\end{document}

